import { NextRequest } from 'next/server'
import { streamText, generateText, CoreMessage } from 'ai'
import { createGoogleGenerativeAI } from '@ai-sdk/google'
import { googleSearchGroundingModels } from '@/features/constants/aiModels'

export const config = {
  runtime: 'edge',
}

export default async function handler(req: NextRequest) {
  if (req.method !== 'POST') {
    return new Response(
      JSON.stringify({
        error: 'Method Not Allowed',
        errorCode: 'METHOD_NOT_ALLOWED',
      }),
      {
        status: 405,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }

  const {
    messages,
    apiKey: apiKeyFromRequest,
    model = 'gemini-2.0-flash',
    stream = false,
    useSearchGrounding = true,
    dynamicRetrievalThreshold,
    temperature = 1.0,
    maxTokens = 4096,
  } = await req.json()

  // ã‚µãƒ¼ãƒãƒ¼å´ã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã‚€ï¼ˆå„ªå…ˆï¼‰
  // ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰æ¥ãŸAPIã‚­ãƒ¼ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
  const apiKey = process.env.GOOGLE_API_KEY || apiKeyFromRequest

  if (!apiKey) {
    return new Response(
      JSON.stringify({ error: 'Empty API Key', errorCode: 'EmptyAPIKey' }),
      {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }

  if (!Array.isArray(messages)) {
    return new Response(
      JSON.stringify({
        error: 'Messages must be an array',
        errorCode: 'InvalidMessages',
      }),
      {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }

  try {
    const aiInstance = createGoogleGenerativeAI({ apiKey })
    const effectiveModel = model || 'gemini-2.0-flash'
    const options: Record<string, any> = {}

    if (
      useSearchGrounding &&
      googleSearchGroundingModels.includes(effectiveModel)
    ) {
      options.useSearchGrounding = true
      // èªå°¾ã«ã€Œã‚µãƒ¼ãƒã€ãŒã¤ã„ã¦ã„ã‚‹å ´åˆã¯ã€ç¢ºå®Ÿã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹
      // dynamicRetrievalConfigã‚’è¨­å®šã—ãªã„ã“ã¨ã§ã€å¸¸ã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
      if (dynamicRetrievalThreshold !== undefined && dynamicRetrievalThreshold !== null && dynamicRetrievalThreshold !== '') {
        options.dynamicRetrievalConfig = {
          dynamicThreshold: dynamicRetrievalThreshold,
        }
      }
      // èªå°¾ã«ã€Œã‚µãƒ¼ãƒã€ãŒã¤ã„ã¦ã„ã‚‹å ´åˆã¯ã€dynamicRetrievalConfigã‚’è¨­å®šã—ãªã„
      // ï¼ˆã“ã‚Œã«ã‚ˆã‚Šã€å¸¸ã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒä½¿ç”¨ã•ã‚Œã‚‹ï¼‰
      const lastUserMessage = messages
        .slice()
        .reverse()
        .find((msg: any) => msg.role === 'user')
      if (lastUserMessage) {
        const messageText = typeof lastUserMessage.content === 'string'
          ? lastUserMessage.content
          : lastUserMessage.content
              ?.filter((part: any) => part.type === 'text')
              ?.map((part: any) => part.text)
              ?.join(' ') || ''
        const trimmedMessage = messageText.trim()
        const searchPattern = /(ã‚µãƒ¼ãƒ|ã•ãƒ¼ã¡|search|Search|SEARCH)(\s*)$/i
        if (searchPattern.test(trimmedMessage)) {
          // èªå°¾ã«ã€Œã‚µãƒ¼ãƒã€ãŒã¤ã„ã¦ã„ã‚‹å ´åˆã¯ã€dynamicRetrievalConfigã‚’å‰Šé™¤ã—ã¦ç¢ºå®Ÿã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
          delete options.dynamicRetrievalConfig
          console.log('[vercel.ts] âœ… èªå°¾ã«ã€Œã‚µãƒ¼ãƒã€æ¤œå‡ºã€dynamicRetrievalConfigã‚’å‰Šé™¤ã—ã¦ç¢ºå®Ÿã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨', {
            messageText: trimmedMessage.substring(Math.max(0, trimmedMessage.length - 30))
          })
        }
      }
    }

    const callAI = async (opts: any) => {
      if (stream) {
        try {
          console.log('[vercel.ts] streamTextå‘¼ã³å‡ºã—é–‹å§‹', {
            model: effectiveModel,
            messagesCount: messages.length,
            temperature,
            maxTokens,
            useSearchGrounding: opts.useSearchGrounding
          })
          const response = await streamText({
            model: aiInstance(effectiveModel, opts),
            messages: messages as CoreMessage[],
            temperature,
            maxTokens,
          })
          console.log('[vercel.ts] streamTextæˆåŠŸã€textStreamã‚’ç›´æ¥ä½¿ç”¨')
          
          // textStreamã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ã€ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½œæˆ
          const textStream = response.textStream
          const encoder = new TextEncoder()
          
          // Vercel AI SDKã®Data Streamå½¢å¼ã«å¤‰æ›
          const dataStream = new ReadableStream({
            async start(controller) {
              try {
                const reader = textStream.getReader()
                let messageId = 0
                let fullText = ''
                let hasSearchGrounding = false
                
                // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®æ¤œå‡ºã‚’å¾…ã¤ï¼ˆæœ€å¤§3ç§’ï¼‰
                if (opts.useSearchGrounding) {
                  try {
                    // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨usageã‚’ä¸¦è¡Œã—ã¦å–å¾—
                    const metadataPromise = Promise.resolve((response as any).experimental_providerMetadata).catch(() => null)
                    const usagePromise = Promise.resolve((response as any).usage).catch(() => null)
                    
                    const [metadata, usage] = await Promise.race([
                      Promise.all([metadataPromise, usagePromise]),
                      new Promise<[any, any]>((resolve) => setTimeout(() => resolve([null, null]), 3000))
                    ])
                    
                    // groundingMetadataãŒå­˜åœ¨ã™ã‚‹ã‹ã€ã¾ãŸã¯ç©ºã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚‚æ¤œå‡ºã™ã‚‹
                    // webSearchQueriesãŒå­˜åœ¨ã™ã‚‹å ´åˆã‚‚æ¤œå‡º
                    const hasGroundingMetadata = !!metadata?.google?.groundingMetadata
                    const hasWebSearchQueries = !!(metadata?.google?.groundingMetadata?.webSearchQueries && metadata.google.groundingMetadata.webSearchQueries.length > 0)
                    const hasSearchQueriesCount = !!(usage && usage.searchQueriesCount !== undefined && usage.searchQueriesCount > 0)
                    
                    hasSearchGrounding = hasGroundingMetadata || hasWebSearchQueries || hasSearchQueriesCount
                    
                    // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šè©³ç´°ã‚’æ–‡å­—åˆ—ã§å‡ºåŠ›
                    console.log(`[vercel.ts] ğŸ” ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºè©³ç´°:`, {
                      hasGroundingMetadata,
                      hasWebSearchQueries,
                      hasSearchQueriesCount,
                      hasSearchGrounding,
                      useSearchGrounding: opts.useSearchGrounding
                    })
                    if (metadata?.google?.groundingMetadata) {
                      console.log(`[vercel.ts] ğŸ“Š groundingMetadataå­˜åœ¨:`, {
                        metadata: JSON.stringify(metadata.google.groundingMetadata).substring(0, 500),
                        webSearchQueries: metadata.google.groundingMetadata.webSearchQueries
                      })
                    }
                    if (usage) {
                      console.log(`[vercel.ts] ğŸ“Š usageå­˜åœ¨:`, {
                        searchQueriesCount: usage.searchQueriesCount,
                        usage: JSON.stringify(usage).substring(0, 300)
                      })
                    }
                    
                    if (hasSearchGrounding) {
                      console.log('[vercel.ts] âœ… ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºæˆåŠŸï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹æ™‚ï¼‰:', {
                        useSearchGrounding: opts.useSearchGrounding,
                        hasSearchGrounding,
                        hasGroundingMetadata: !!metadata?.google?.groundingMetadata,
                        hasWebSearchQueries: !!(metadata?.google?.groundingMetadata?.webSearchQueries && metadata.google.groundingMetadata.webSearchQueries.length > 0),
                        webSearchQueriesCount: metadata?.google?.groundingMetadata?.webSearchQueries?.length || 0,
                        searchQueriesCount: usage?.searchQueriesCount
                      })
                    } else {
                      console.log('[vercel.ts] âŒ ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æœªæ¤œå‡ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹æ™‚ï¼‰:', {
                        useSearchGrounding: opts.useSearchGrounding,
                        hasSearchGrounding: false,
                        hasGroundingMetadata: !!metadata?.google?.groundingMetadata,
                        hasWebSearchQueries: !!(metadata?.google?.groundingMetadata?.webSearchQueries && metadata.google.groundingMetadata.webSearchQueries.length > 0),
                        webSearchQueriesCount: metadata?.google?.groundingMetadata?.webSearchQueries?.length || 0,
                        searchQueriesCount: usage?.searchQueriesCount
                      })
                    }
                  } catch (error) {
                    console.error('[vercel.ts] ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹æ™‚ï¼‰:', error)
                  }
                }
                
                // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®æƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦é€ä¿¡
                if (hasSearchGrounding) {
                  controller.enqueue(encoder.encode(`f:{"messageId":"${messageId++}","hasSearchGrounding":true}\n`))
                  console.log('[vercel.ts] ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡')
                } else {
                  controller.enqueue(encoder.encode(`f:{"messageId":"${messageId++}"}\n`))
                }
                
                while (true) {
                  const { done, value } = await reader.read()
                  if (done) {
                    // çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
                    controller.enqueue(encoder.encode(`d:{}\n`))
                    controller.close()
                    break
                  }
                  
                  // ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’Data Streamå½¢å¼ã§é€ä¿¡
                  fullText += value
                  const escapedValue = value.replace(/\\/g, '\\\\').replace(/"/g, '\\"')
                  controller.enqueue(encoder.encode(`0:"${escapedValue}"\n`))
                }
              } catch (error) {
                console.error('[vercel.ts] textStreamå‡¦ç†ã‚¨ãƒ©ãƒ¼:', error)
                // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
                const errorMessage = error instanceof Error ? error.message : 'An error occurred.'
                controller.enqueue(encoder.encode(`3:"${errorMessage}"\n`))
                controller.close()
              }
            }
          })
          
          return new Response(dataStream, {
            headers: {
              'Content-Type': 'text/plain; charset=utf-8',
            },
          })
        } catch (streamError: any) {
          console.error('[vercel.ts] streamTextã‚¨ãƒ©ãƒ¼:', streamError)
          console.error('[vercel.ts] ã‚¨ãƒ©ãƒ¼è©³ç´°:', {
            message: streamError?.message,
            stack: streamError?.stack,
            cause: streamError?.cause,
            name: streamError?.name
          })
          // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã§è¿”ã™
          const errorMessage = streamError?.message || 'An error occurred.'
          return new Response(
            `3:"${errorMessage}"\n`,
            {
              status: 200,
              headers: {
                'Content-Type': 'text/plain; charset=utf-8',
              },
            }
          )
        }
      }

      const result = await generateText({
        model: aiInstance(effectiveModel, opts),
        messages: messages as CoreMessage[],
        temperature,
        maxTokens,
      })

      return new Response(JSON.stringify({ text: result.text }), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    try {
      return await callAI(options)
    } catch (firstError) {
      // Search Grounding ãŒæœ‰åŠ¹ã ã£ãŸå ´åˆã®ã¿ã€ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
      if (options.useSearchGrounding) {
        console.warn('[vercel.ts] Search Grounding failed, retrying without search...', firstError)
        delete options.useSearchGrounding
        delete options.dynamicRetrievalConfig
        return await callAI(options)
      }
      throw firstError
    }

  } catch (error) {
    console.error('Error in Gemini API call:', error)

    return new Response(
      JSON.stringify({
        error: 'Unexpected Error',
        errorCode: 'AIAPIError',
      }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }
}

