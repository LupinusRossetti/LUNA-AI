import { NextRequest } from 'next/server'
import { streamText, generateText, CoreMessage } from 'ai'
import { createGoogleGenerativeAI } from '@ai-sdk/google'
import { googleSearchGroundingModels } from '@/features/constants/aiModels'

export const config = {
  runtime: 'edge',
}

export default async function handler(req: NextRequest) {
  if (req.method !== 'POST') {
    return new Response(
      JSON.stringify({
        error: 'Method Not Allowed',
        errorCode: 'METHOD_NOT_ALLOWED',
      }),
      {
        status: 405,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }

  const {
    messages,
    apiKey: apiKeyFromRequest,
    model = 'gemini-2.0-flash',
    stream = false,
    useSearchGrounding = true,
    dynamicRetrievalThreshold,
    temperature = 1.0,
    maxTokens = 4096,
  } = await req.json()

  // ã‚µãƒ¼ãƒãƒ¼å´ã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã‚€ï¼ˆå„ªå…ˆï¼‰
  // ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰æ¥ãŸAPIã‚­ãƒ¼ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã—ã¦ä½¿ç”¨
  const apiKey = process.env.GOOGLE_API_KEY || apiKeyFromRequest

  if (!apiKey) {
    return new Response(
      JSON.stringify({ error: 'Empty API Key', errorCode: 'EmptyAPIKey' }),
      {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }

  if (!Array.isArray(messages)) {
    return new Response(
      JSON.stringify({
        error: 'Messages must be an array',
        errorCode: 'InvalidMessages',
      }),
      {
        status: 400,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }

  try {
    const aiInstance = createGoogleGenerativeAI({ apiKey })
    const effectiveModel = model || 'gemini-2.0-flash'
    const options: Record<string, any> = {}

    console.log('[vercel.ts] ğŸ” ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¡ä»¶ãƒã‚§ãƒƒã‚¯:', {
      useSearchGrounding,
      effectiveModel,
      isInGoogleSearchGroundingModels: googleSearchGroundingModels.includes(effectiveModel),
      googleSearchGroundingModels: googleSearchGroundingModels
    })

    if (
      useSearchGrounding &&
      googleSearchGroundingModels.includes(effectiveModel)
    ) {
      options.useSearchGrounding = true
      // èªå°¾ã«ã€Œã‚µãƒ¼ãƒã€ãŒã¤ã„ã¦ã„ã‚‹å ´åˆã¯ã€ç¢ºå®Ÿã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã™ã‚‹
      // dynamicRetrievalConfigã‚’è¨­å®šã—ãªã„ã“ã¨ã§ã€å¸¸ã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
      if (dynamicRetrievalThreshold !== undefined && dynamicRetrievalThreshold !== null && dynamicRetrievalThreshold !== '') {
        options.dynamicRetrievalConfig = {
          dynamicThreshold: dynamicRetrievalThreshold,
        }
      }
      // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦ãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œå‡ºã™ã‚‹é–¢æ•°
      const shouldForceSearchGrounding = (messageText: string): boolean => {
        const trimmedMessage = messageText.trim().toLowerCase()

        // æ˜ç¤ºçš„ãªã€Œã‚µãƒ¼ãƒã€æ¤œå‡º
        const searchPattern = /(ã‚µãƒ¼ãƒ|ã•ãƒ¼ã¡|search)(\s*)$/i
        if (searchPattern.test(trimmedMessage)) {
          console.log('[vercel.ts] âœ… æ˜ç¤ºçš„ãªã€Œã‚µãƒ¼ãƒã€æ¤œå‡º')
          return true
        }

        // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦ãã†ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        const searchKeywords = [
          // æœ€æ–°æƒ…å ±é–¢é€£
          'æœ€æ–°', 'æœ€æ–°æƒ…å ±', 'ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ', 'update', 'æ–°æ©Ÿèƒ½', 'æ–°æƒ…å ±',
          // æ”»ç•¥æƒ…å ±é–¢é€£
          'æ”»ç•¥', 'æ”»ç•¥æ³•', 'æ”»ç•¥æ–¹æ³•', 'æ”»ç•¥æƒ…å ±', 'æ”»ç•¥ã‚¬ã‚¤ãƒ‰',
          // ã‚²ãƒ¼ãƒ æƒ…å ±é–¢é€£
          'ãƒ‰ãƒ©ã‚¯ã‚¨', 'ãƒ‰ãƒ©ã‚´ãƒ³ã‚¯ã‚¨ã‚¹ãƒˆ', 'dq', 'dragon quest',
          'ãƒã‚±ãƒ¢ãƒ³', 'pokemon', 'ãƒ•ã‚¡ã‚¤ãƒŠãƒ«ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼', 'ff', 'final fantasy',
          'ãƒ¢ãƒ³ã‚¹ã‚¿ãƒ¼ãƒãƒ³ã‚¿ãƒ¼', 'mh', 'monster hunter',
          // ã‚¢ãƒ‹ãƒ¡æƒ…å ±é–¢é€£
          'ã‚¢ãƒ‹ãƒ¡', 'anime', 'æ”¾é€', 'ã‚­ãƒ£ã‚¹ãƒˆ', 'cast',
          // æµè¡Œãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰é–¢é€£
          'æµè¡Œ', 'ãƒˆãƒ¬ãƒ³ãƒ‰', 'trend', 'è©±é¡Œ', 'ãƒã‚º', 'buzz',
          // ã‚³ã‚¹ãƒ¡é–¢é€£
          'ã‚³ã‚¹ãƒ¡', 'cosme', 'åŒ–ç²§å“', 'ãƒ¡ã‚¤ã‚¯', 'makeup',
          // æ™‚äº‹é–¢é€£
          'ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'news', 'æ™‚äº‹', 'ç¤¾ä¼šæƒ…å‹¢',
          // æƒ…å ±å–å¾—ã‚’æ±‚ã‚ã‚‹è¡¨ç¾
          'æ•™ãˆã¦', 'çŸ¥ã‚ŠãŸã„', 'æƒ…å ±', 'è©³ã—ã', 'è©³ç´°',
          'ã„ã¤', 'ã©ã“', 'èª°', 'ä½•', 'ã©ã†', 'ãªãœ', 'ãªã‚“ã§',
          // å›ºæœ‰åè©ã®æ¤œç´¢ãŒå¿…è¦ãã†ãªè¡¨ç¾
          'ã¨ã¯', 'ã£ã¦ä½•', 'ã«ã¤ã„ã¦', 'ã«ã¤ã„ã¦æ•™ãˆã¦'
        ]

        // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        for (const keyword of searchKeywords) {
          if (trimmedMessage.includes(keyword.toLowerCase())) {
            console.log('[vercel.ts] âœ… ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º:', keyword)
            return true
          }
        }

        console.log('[vercel.ts] âŒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æœªæ¤œå‡º')
        return false
      }

      // èªå°¾ã«ã€Œã‚µãƒ¼ãƒã€ãŒã¤ã„ã¦ã„ã‚‹å ´åˆã€ã¾ãŸã¯å†…å®¹ã‹ã‚‰ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆã¯ã€dynamicRetrievalConfigã‚’å‰Šé™¤ã—ã¦ç¢ºå®Ÿã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
      const lastUserMessage = messages
        .slice()
        .reverse()
        .find((msg: any) => msg.role === 'user')
      if (lastUserMessage) {
        const messageText = typeof lastUserMessage.content === 'string'
          ? lastUserMessage.content
          : lastUserMessage.content
            ?.filter((part: any) => part.type === 'text')
            ?.map((part: any) => part.text)
            ?.join(' ') || ''
        const shouldForce = shouldForceSearchGrounding(messageText)
        console.log('[vercel.ts] ğŸ” ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°åˆ¤å®š:', {
          messageText: messageText.trim().substring(Math.max(0, messageText.trim().length - 50)),
          shouldForce,
          hasDynamicRetrievalConfig: !!options.dynamicRetrievalConfig
        })
        if (shouldForce) {
          // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦ã¨åˆ¤å®šã•ã‚ŒãŸå ´åˆã¯ã€dynamicRetrievalConfigã‚’å‰Šé™¤ã—ã¦ç¢ºå®Ÿã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨
          delete options.dynamicRetrievalConfig
          console.log('[vercel.ts] âœ… ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°å¿…è¦ã¨åˆ¤å®šã€dynamicRetrievalConfigã‚’å‰Šé™¤ã—ã¦ç¢ºå®Ÿã«ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨', {
            messageText: messageText.trim().substring(Math.max(0, messageText.trim().length - 50)),
            options: JSON.stringify(options)
          })
        }
      }
      console.log('[vercel.ts] ğŸ“Š æœ€çµ‚çš„ãªoptions:', {
        useSearchGrounding: options.useSearchGrounding,
        hasDynamicRetrievalConfig: !!options.dynamicRetrievalConfig,
        dynamicRetrievalConfig: options.dynamicRetrievalConfig
      })
    }

    // ------------------------------------------------------------------
    // [TWO-STAGE PROCESSING] Search Grounding + Dialogue Generation
    // ------------------------------------------------------------------
    // æ›ã‘åˆã„ãƒ¢ãƒ¼ãƒ‰æ™‚ã®2æ®µéšå‡¦ç†ï¼š
    // ã‚±ãƒ¼ã‚¹1: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºå®Ÿã«ä½¿ç”¨ â†’ Stage 1: æƒ…å ±å–å¾—ã€Stage 2: æ›ã‘åˆã„ç”Ÿæˆ
    // ã‚±ãƒ¼ã‚¹2: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºå®Ÿã«ä¸ä½¿ç”¨ â†’ Stage 1: æ›ã‘åˆã„ç”Ÿæˆï¼ˆ1æ®µéšã®ã¿ï¼‰
    // ã‚±ãƒ¼ã‚¹3: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°å‹•çš„åˆ¤å®š â†’ Stage 1: ä½¿ç”¨æ™‚ã¯æƒ…å ±å–å¾—ã€ä¸ä½¿ç”¨æ™‚ã¯æ›ã‘åˆã„ç”Ÿæˆã€Stage 2: ä½¿ç”¨æ™‚ã®ã¿å®Ÿè¡Œ
    // ------------------------------------------------------------------

    // æ›ã‘åˆã„ãƒ¢ãƒ¼ãƒ‰ã®åˆ¤å®š
    const isDialogueMode = process.env.NEXT_PUBLIC_DIALOGUE_MODE === 'true'
    
    // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®ä½¿ç”¨çŠ¶æ³ã‚’åˆ¤å®š
    const hasDynamicRetrievalConfig = !!options.dynamicRetrievalConfig
    const willUseSearchGrounding = options.useSearchGrounding && !hasDynamicRetrievalConfig // dynamicRetrievalConfigãŒãªã„å ´åˆã¯ç¢ºå®Ÿã«ä½¿ç”¨
    const mightUseSearchGrounding = options.useSearchGrounding && hasDynamicRetrievalConfig // dynamicRetrievalConfigãŒã‚ã‚‹å ´åˆã¯å‹•çš„åˆ¤å®š
    
    console.log('[vercel.ts] ğŸ” 2æ®µéšå‡¦ç†æ¡ä»¶ãƒã‚§ãƒƒã‚¯:', {
      useSearchGrounding: options.useSearchGrounding,
      hasDynamicRetrievalConfig,
      willUseSearchGrounding,
      mightUseSearchGrounding,
      isDialogueMode,
      stream
    })

    // XMLã‚¿ã‚°ã‚’é™¤å»ã™ã‚‹é–¢æ•°
    const removeXmlTags = (text: string): string => {
      // XMLã‚¿ã‚°ï¼ˆ<A>...</A>, <B>...</B>ãªã©ï¼‰ã‚’é™¤å»
      return text
        .replace(/<[^>]+>/g, '') // é–‹å§‹ã‚¿ã‚°ã¨çµ‚äº†ã‚¿ã‚°ã‚’é™¤å»
        .replace(/&lt;/g, '<') // HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’å¾©å…ƒ
        .replace(/&gt;/g, '>')
        .replace(/&amp;/g, '&')
        .trim()
    }

    // æ›ã‘åˆã„ãƒ¢ãƒ¼ãƒ‰ã‹ã¤ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã®å ´åˆã®ã¿2æ®µéšå‡¦ç†ã‚’æ¤œè¨
    if (isDialogueMode && stream) {
      // å…ƒã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—
      const lastUserMessageIndex = messages
        .slice()
        .reverse()
        .findIndex((msg: any) => msg.role === 'user')

      if (lastUserMessageIndex !== -1) {
        const realIndex = messages.length - 1 - lastUserMessageIndex
        const lastUserMessage = messages[realIndex]

        const originalContent = typeof lastUserMessage.content === 'string'
          ? lastUserMessage.content
          : (Array.isArray(lastUserMessage.content)
            ? lastUserMessage.content.map((c: any) => c.text || '').join('')
            : '')

        try {
          // === ã‚±ãƒ¼ã‚¹1: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºå®Ÿã«ä½¿ç”¨ ===
          if (willUseSearchGrounding) {
            console.log('[vercel.ts] ğŸ”„ ã‚±ãƒ¼ã‚¹1: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºå®Ÿã«ä½¿ç”¨ â†’ 2æ®µéšå‡¦ç†')

            // Stage 1: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã§æƒ…å ±å–å¾—ï¼ˆãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼‰
            console.log('[vercel.ts] ğŸ“¡ Stage 1: æ¤œç´¢ã§æœ€æ–°æƒ…å ±ã‚’å–å¾—ä¸­...')

            const stage1SystemPrompt = `ã‚ãªãŸã¯æœ€æ–°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€Google Search Groundingã‚’ä½¿ç”¨ã—ã¦æœ€æ–°ã®æ­£ç¢ºãªæƒ…å ±ã‚’å–å¾—ã—ã€ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‚„æ›ã‘åˆã„å½¢å¼ã¯ä¸è¦ã§ã™ã€‚ç´”ç²‹ã«æƒ…å ±ã®ã¿ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`

            const stage1Messages: CoreMessage[] = [
              { role: 'system', content: stage1SystemPrompt },
              { role: 'user', content: originalContent }
            ]

            const searchResponse = await generateText({
              model: aiInstance(effectiveModel, { useSearchGrounding: true }),
              messages: stage1Messages,
              temperature: 0.7,
              maxTokens: 2048,
            })

            let searchResult = searchResponse.text
            // XMLã‚¿ã‚°ãŒå«ã¾ã‚Œã¦ã„ãŸå ´åˆã¯é™¤å»
            searchResult = removeXmlTags(searchResult)

            console.log('[vercel.ts] âœ… Stage 1 å®Œäº†: æƒ…å ±å–å¾—æˆåŠŸ', {
              length: searchResult.length,
              preview: searchResult.substring(0, 100)
            })

            // Stage 2: æ›ã‘åˆã„ç”Ÿæˆï¼ˆã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‚’é©ç”¨ï¼‰
            console.log('[vercel.ts] ğŸ­ Stage 2: æ›ã‘åˆã„ã‚’ç”Ÿæˆä¸­...')

            const { SYSTEM_PROMPT } = await import('@/features/constants/systemPromptConstants')
            const { getCharacterNames } = await import('@/utils/characterNames')
            const systemPromptA = process.env.NEXT_PUBLIC_SYSTEM_PROMPT_A || SYSTEM_PROMPT
            const systemPromptB = process.env.NEXT_PUBLIC_SYSTEM_PROMPT_B || SYSTEM_PROMPT
            const characterNames = getCharacterNames()
            const characterAName = characterNames.characterA.fullName
            const characterBName = characterNames.characterB.fullName
            const characterANickname = characterNames.characterA.nickname
            const characterBNickname = characterNames.characterB.nickname

            const stage2SystemPrompt = `[${characterAName}ï¼ˆAï¼‰ã®è¨­å®š]
${systemPromptA}

[${characterBName}ï¼ˆBï¼‰ã®è¨­å®š]
${systemPromptB}

[æ›ã‘åˆã„ãƒ¢ãƒ¼ãƒ‰]
- ${characterAName}ï¼ˆAï¼‰ã¨${characterBName}ï¼ˆBï¼‰ã®æ›ã‘åˆã„ã‚’ç”Ÿæˆ
- å¿…ãšäº¤äº’ã«è©±ã™ï¼ˆ${characterANickname}ã‹ã‚‰é–‹å§‹ï¼‰
- XMLå½¢å¼ã®ã¿å‡ºåŠ›ï¼ˆå‰ç½®ãä¸è¦ï¼‰`

            const dialoguePrompt = `æœ€æ–°æƒ…å ±ã‚’å…ƒã«ã€${characterAName}ï¼ˆAï¼‰ã¨${characterBName}ï¼ˆBï¼‰ã®æ›ã‘åˆã„ã‚’XMLå½¢å¼ã§ä½œæˆã€‚

ã€æœ€æ–°æƒ…å ±ã€‘
${searchResult}

ã€å‡ºåŠ›ä¾‹ã€‘
<A emotion="happy">ã‚»ãƒªãƒ•</A>
<B emotion="relaxed">ã‚»ãƒªãƒ•</B>
<A emotion="excited">ã‚»ãƒªãƒ•</A>
<B emotion="happy">ã‚»ãƒªãƒ•</B>
<A emotion="relaxed">ã‚»ãƒªãƒ•</A>
<B emotion="surprised">ã‚»ãƒªãƒ•</B>
<A emotion="happy">ã‚»ãƒªãƒ•</A>

ã€ãƒ«ãƒ¼ãƒ«ã€‘
1. æœ€ä½7ã‚¿ãƒ¼ãƒ³ä»¥ä¸Šï¼ˆAâ†’Bâ†’Aâ†’Bâ†’Aâ†’Bâ†’Aï¼‰
2. äº¤äº’ã«è©±ã™
3. æ„Ÿæƒ…ã‚¿ã‚°ï¼ˆemotionï¼‰ã‚’ä»˜ã‘ã‚‹
4. æœ€æ–°æƒ…å ±ã‚’è‡ªç„¶ã«ä¼šè©±ã«ç¹”ã‚Šè¾¼ã‚€
5. 500æ–‡å­—ä»¥å†…ã§7ã‚¿ãƒ¼ãƒ³ä»¥ä¸Šç”Ÿæˆ

ã€é‡è¦ã€‘
- XMLã‚¿ã‚°ã®ã¿å‡ºåŠ›ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ä¸å¯ï¼‰
- æœ€åˆã¯ã€Œ<Aã€ã§é–‹å§‹`

            const stage2Messages: CoreMessage[] = [
              { role: 'system', content: stage2SystemPrompt },
              { role: 'user', content: dialoguePrompt }
            ]

            const response = await streamText({
              model: aiInstance(effectiveModel, {}),
              messages: stage2Messages,
              temperature,
              maxTokens: 500, // 500æ–‡å­—ä»¥å†…ã®åˆ¶ç´„ã‚’å³å®ˆ
            })

            console.log('[vercel.ts] âœ… Stage 2 é–‹å§‹: æ›ã‘åˆã„ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­')

            const textStream = response.textStream
            const encoder = new TextEncoder()

            const dataStream = new ReadableStream({
              async start(controller) {
                try {
                  const reader = textStream.getReader()
                  let messageId = 0

                  const responseMetadata: any = {
                    messageId: messageId++,
                    hasSearchGrounding: true,
                    twoStageProcessing: true,
                    debug: {
                      stage1Length: searchResult.length,
                      stage1Preview: searchResult.substring(0, 50)
                    }
                  }
                  controller.enqueue(encoder.encode(`f:${JSON.stringify(responseMetadata)}\n`))

                  while (true) {
                    const { done, value } = await reader.read()
                    if (done) {
                      controller.enqueue(encoder.encode(`d:{}\n`))
                      controller.close()
                      break
                    }

                    // æ–‡å­—åŒ–ã‘ã‚’é˜²ããŸã‚ã€ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†ã‚’æ”¹å–„
                    // JSON.stringifyã‚’ä½¿ç”¨ã—ã¦æ­£ã—ãã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
                    const escapedValue = JSON.stringify(value)
                    controller.enqueue(encoder.encode(`0:${escapedValue}\n`))
                  }
                } catch (error) {
                  console.error('[vercel.ts] 2æ®µéšå‡¦ç†ã‚¨ãƒ©ãƒ¼:', error)
                  const errorMessage = error instanceof Error ? error.message : 'An error occurred.'
                  controller.enqueue(encoder.encode(`3:"${errorMessage}"\n`))
                  controller.close()
                }
              }
            })

            return new Response(dataStream, {
              headers: {
                'Content-Type': 'text/plain; charset=utf-8',
              },
            })
          }

          // === ã‚±ãƒ¼ã‚¹2: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºå®Ÿã«ä¸ä½¿ç”¨ ===
          // æ›ã‘åˆã„ã‚’1æ®µéšã§ç”Ÿæˆï¼ˆ7ã‚¿ãƒ¼ãƒ³ä»¥ä¸Š500æ–‡å­—ä»¥å†…ï¼‰
          if (!willUseSearchGrounding && !mightUseSearchGrounding) {
            console.log('[vercel.ts] ğŸ”„ ã‚±ãƒ¼ã‚¹2: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºå®Ÿã«ä¸ä½¿ç”¨ â†’ 1æ®µéšã§æ›ã‘åˆã„ç”Ÿæˆ')
            
            // é€šå¸¸ã®å‡¦ç†ï¼ˆcallAIï¼‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            // callAIå†…ã§æ›ã‘åˆã„ãƒ¢ãƒ¼ãƒ‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé©ç”¨ã•ã‚Œã‚‹
          }

          // === ã‚±ãƒ¼ã‚¹3: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°å‹•çš„åˆ¤å®š ===
          if (mightUseSearchGrounding) {
            console.log('[vercel.ts] ğŸ”„ ã‚±ãƒ¼ã‚¹3: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°å‹•çš„åˆ¤å®š â†’ æ¡ä»¶ä»˜ã2æ®µéšå‡¦ç†')

            // Stage 1: ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œï¼ˆå‹•çš„åˆ¤å®šï¼‰
            const stage1SystemPrompt = `ã‚ãªãŸã¯æœ€æ–°æƒ…å ±ã‚’å–å¾—ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦Google Search Groundingã‚’ä½¿ç”¨ã—ã¦æœ€æ–°ã®æ­£ç¢ºãªæƒ…å ±ã‚’å–å¾—ã—ã€ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šã‚„æ›ã‘åˆã„å½¢å¼ã¯ä¸è¦ã§ã™ã€‚ç´”ç²‹ã«æƒ…å ±ã®ã¿ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚`

            const stage1Messages: CoreMessage[] = [
              { role: 'system', content: stage1SystemPrompt },
              { role: 'user', content: originalContent }
            ]

            // generateTextã‚’ä½¿ç”¨ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            const stage1Response = await generateText({
              model: aiInstance(effectiveModel, options), // dynamicRetrievalConfigã‚’å«ã‚€
              messages: stage1Messages,
              temperature: 0.7,
              maxTokens: 2048,
            })

            let stage1Result = stage1Response.text

            // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒä½¿ç”¨ã•ã‚ŒãŸã‹åˆ¤å®š
            let hasSearchGroundingUsed = false
            if (stage1Response.experimental_providerMetadata?.google?.groundingMetadata) {
              hasSearchGroundingUsed = true
            } else if (stage1Response.usage?.searchQueriesCount && stage1Response.usage.searchQueriesCount > 0) {
              hasSearchGroundingUsed = true
            }

            console.log('[vercel.ts] ğŸ“Š Stage 1 çµæœ:', {
              hasSearchGroundingUsed,
              resultLength: stage1Result.length,
              preview: stage1Result.substring(0, 100)
            })

            // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒä½¿ç”¨ã•ã‚ŒãŸå ´åˆã®ã¿Stage 2ã‚’å®Ÿè¡Œ
            if (hasSearchGroundingUsed) {
              console.log('[vercel.ts] âœ… ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ä½¿ç”¨ã‚’æ¤œå‡º â†’ Stage 2å®Ÿè¡Œ')

              // XMLã‚¿ã‚°ãŒå«ã¾ã‚Œã¦ã„ãŸå ´åˆã¯é™¤å»
              let cleanedResult = removeXmlTags(stage1Result)

              // Stage 2: æ›ã‘åˆã„ç”Ÿæˆ
              console.log('[vercel.ts] ğŸ­ Stage 2: æ›ã‘åˆã„ã‚’ç”Ÿæˆä¸­...')

              const { SYSTEM_PROMPT } = await import('@/features/constants/systemPromptConstants')
              const { getCharacterNames } = await import('@/utils/characterNames')
              const systemPromptA = process.env.NEXT_PUBLIC_SYSTEM_PROMPT_A || SYSTEM_PROMPT
              const systemPromptB = process.env.NEXT_PUBLIC_SYSTEM_PROMPT_B || SYSTEM_PROMPT
              const characterNames = getCharacterNames()
              const characterAName = characterNames.characterA.fullName
              const characterBName = characterNames.characterB.fullName
              const characterANickname = characterNames.characterA.nickname
              const characterBNickname = characterNames.characterB.nickname

              const stage2SystemPrompt = `[${characterAName}ï¼ˆAï¼‰ã®è¨­å®š]
${systemPromptA}

[${characterBName}ï¼ˆBï¼‰ã®è¨­å®š]
${systemPromptB}

[æ›ã‘åˆã„ãƒ¢ãƒ¼ãƒ‰]
- ${characterAName}ï¼ˆAï¼‰ã¨${characterBName}ï¼ˆBï¼‰ã®æ›ã‘åˆã„ã‚’ç”Ÿæˆ
- å¿…ãšäº¤äº’ã«è©±ã™ï¼ˆ${characterANickname}ã‹ã‚‰é–‹å§‹ï¼‰
- XMLå½¢å¼ã®ã¿å‡ºåŠ›ï¼ˆå‰ç½®ãä¸è¦ï¼‰`

              const dialoguePrompt = `æœ€æ–°æƒ…å ±ã‚’å…ƒã«ã€${characterAName}ï¼ˆAï¼‰ã¨${characterBName}ï¼ˆBï¼‰ã®æ›ã‘åˆã„ã‚’XMLå½¢å¼ã§ä½œæˆã€‚

ã€æœ€æ–°æƒ…å ±ã€‘
${cleanedResult}

ã€å‡ºåŠ›ä¾‹ã€‘
<A emotion="happy">ã‚»ãƒªãƒ•</A>
<B emotion="relaxed">ã‚»ãƒªãƒ•</B>
<A emotion="excited">ã‚»ãƒªãƒ•</A>
<B emotion="happy">ã‚»ãƒªãƒ•</B>
<A emotion="relaxed">ã‚»ãƒªãƒ•</A>
<B emotion="surprised">ã‚»ãƒªãƒ•</B>
<A emotion="happy">ã‚»ãƒªãƒ•</A>

ã€ãƒ«ãƒ¼ãƒ«ã€‘
1. æœ€ä½7ã‚¿ãƒ¼ãƒ³ä»¥ä¸Šï¼ˆAâ†’Bâ†’Aâ†’Bâ†’Aâ†’Bâ†’Aï¼‰
2. äº¤äº’ã«è©±ã™
3. æ„Ÿæƒ…ã‚¿ã‚°ï¼ˆemotionï¼‰ã‚’ä»˜ã‘ã‚‹
4. æœ€æ–°æƒ…å ±ã‚’è‡ªç„¶ã«ä¼šè©±ã«ç¹”ã‚Šè¾¼ã‚€
5. 500æ–‡å­—ä»¥å†…ã§7ã‚¿ãƒ¼ãƒ³ä»¥ä¸Šç”Ÿæˆ

ã€é‡è¦ã€‘
- XMLã‚¿ã‚°ã®ã¿å‡ºåŠ›ï¼ˆãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ä¸å¯ï¼‰
- æœ€åˆã¯ã€Œ<Aã€ã§é–‹å§‹`

              const stage2Messages: CoreMessage[] = [
                { role: 'system', content: stage2SystemPrompt },
                { role: 'user', content: dialoguePrompt }
              ]

              const response = await streamText({
                model: aiInstance(effectiveModel, {}),
                messages: stage2Messages,
                temperature,
                maxTokens: 800,
              })

              console.log('[vercel.ts] âœ… Stage 2 é–‹å§‹: æ›ã‘åˆã„ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­')

              const textStream = response.textStream
              const encoder = new TextEncoder()

              const dataStream = new ReadableStream({
                async start(controller) {
                  try {
                    const reader = textStream.getReader()
                    let messageId = 0

                    const responseMetadata: any = {
                      messageId: messageId++,
                      hasSearchGrounding: true,
                      twoStageProcessing: true,
                      debug: {
                        stage1Length: cleanedResult.length,
                        stage1Preview: cleanedResult.substring(0, 50),
                        hasSearchGroundingUsed
                      }
                    }
                    controller.enqueue(encoder.encode(`f:${JSON.stringify(responseMetadata)}\n`))

                    while (true) {
                      const { done, value } = await reader.read()
                      if (done) {
                        controller.enqueue(encoder.encode(`d:{}\n`))
                        controller.close()
                        break
                      }

                      const escapedValue = JSON.stringify(value)
                      controller.enqueue(encoder.encode(`0:${escapedValue}\n`))
                    }
                  } catch (error) {
                    console.error('[vercel.ts] 2æ®µéšå‡¦ç†ã‚¨ãƒ©ãƒ¼:', error)
                    const errorMessage = error instanceof Error ? error.message : 'An error occurred.'
                    controller.enqueue(encoder.encode(`3:"${errorMessage}"\n`))
                    controller.close()
                  }
                }
              })

              return new Response(dataStream, {
                headers: {
                  'Content-Type': 'text/plain; charset=utf-8',
                },
              })
            } else {
              // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒä½¿ç”¨ã•ã‚Œãªã‹ã£ãŸå ´åˆã€é€šå¸¸ã®å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
              // Stage 1ã®çµæœã¯æ›ã‘åˆã„å½¢å¼ã§è¿”ã£ã¦ãã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€é€šå¸¸ã®å‡¦ç†ã§å‡¦ç†ã•ã‚Œã‚‹
              console.log('[vercel.ts] â„¹ï¸ ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æœªä½¿ç”¨ â†’ é€šå¸¸å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯')
            }
          }

        } catch (error) {
          console.error('[vercel.ts] 2æ®µéšå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:', error)
          console.log('[vercel.ts] âš ï¸ é€šå¸¸å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯')
        }
      }
    }

    // ã‚±ãƒ¼ã‚¹2ï¼ˆã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ç¢ºå®Ÿã«ä¸ä½¿ç”¨ï¼‰ã‚„
    // ã‚±ãƒ¼ã‚¹3ã§ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãŒä½¿ç”¨ã•ã‚Œãªã‹ã£ãŸå ´åˆã¯ã€
    // é€šå¸¸ã®å‡¦ç†ï¼ˆcallAIï¼‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯


    const callAI = async (opts: any) => {
      if (stream) {
        try {
          console.log('[vercel.ts] streamTextå‘¼ã³å‡ºã—é–‹å§‹', {
            model: effectiveModel,
            messagesCount: messages.length,
            temperature,
            maxTokens,
            useSearchGrounding: opts.useSearchGrounding
          })
          const response = await streamText({
            model: aiInstance(effectiveModel, opts),
            messages: messages as CoreMessage[],
            temperature,
            maxTokens,
          })
          console.log('[vercel.ts] streamTextæˆåŠŸã€textStreamã‚’ç›´æ¥ä½¿ç”¨')
          console.log('[vercel.ts] responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ:', {
            hasExperimentalProviderMetadata: !!(response as any).experimental_providerMetadata,
            hasUsage: !!(response as any).usage,
            responseType: typeof response,
            responseKeys: Object.keys(response || {})
          })

          // textStreamã‚’ç›´æ¥ä½¿ç”¨ã—ã¦ã€ã‚«ã‚¹ã‚¿ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ä½œæˆ
          const textStream = response.textStream
          const encoder = new TextEncoder()

          // Vercel AI SDKã®Data Streamå½¢å¼ã«å¤‰æ›
          const dataStream = new ReadableStream({
            async start(controller) {
              try {
                const reader = textStream.getReader()
                let messageId = 0
                let fullText = ''
                let hasSearchGrounding = false

                // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®æ¤œå‡ºã‚’å¾…ã¤ï¼ˆæœ€å¤§5ç§’ï¼‰
                let metadata: any = null
                let usage: any = null
                if (opts.useSearchGrounding) {
                  try {
                    // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨usageã‚’ä¸¦è¡Œã—ã¦å–å¾—
                    // responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦ã¿ã‚‹
                    console.log('[vercel.ts] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹:', {
                      hasExperimentalProviderMetadata: !!(response as any).experimental_providerMetadata,
                      hasUsage: !!(response as any).usage,
                      responseKeys: Object.keys(response || {})
                    })

                    // å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ ãŒé–‹å§‹ã•ã‚Œã‚‹ã¾ã§å¾…ã¤ï¼‰
                    await new Promise(resolve => setTimeout(resolve, 1000))

                    const metadataPromise = Promise.resolve((response as any).experimental_providerMetadata).catch(() => null)
                    const usagePromise = Promise.resolve((response as any).usage).catch(() => null)

                    const [metadataResult, usageResult] = await Promise.race([
                      Promise.all([metadataPromise, usagePromise]),
                      new Promise<[any, any]>((resolve) => setTimeout(() => resolve([null, null]), 5000))
                    ])
                    metadata = metadataResult
                    usage = usageResult

                    console.log('[vercel.ts] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœ:', {
                      metadata: metadata ? 'ã‚ã‚Š' : 'ãªã—',
                      usage: usage ? 'ã‚ã‚Š' : 'ãªã—',
                      metadataKeys: metadata ? Object.keys(metadata) : [],
                      usageKeys: usage ? Object.keys(usage) : []
                    })

                    // groundingMetadataãŒå­˜åœ¨ã™ã‚‹ã‹ã€ã¾ãŸã¯ç©ºã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚‚æ¤œå‡ºã™ã‚‹
                    // webSearchQueriesãŒå­˜åœ¨ã™ã‚‹å ´åˆã‚‚æ¤œå‡º
                    const hasGroundingMetadata = !!metadata?.google?.groundingMetadata
                    const hasWebSearchQueries = !!(metadata?.google?.groundingMetadata?.webSearchQueries && metadata.google.groundingMetadata.webSearchQueries.length > 0)
                    const hasSearchQueriesCount = !!(usage && usage.searchQueriesCount !== undefined && usage.searchQueriesCount > 0)

                    hasSearchGrounding = hasGroundingMetadata || hasWebSearchQueries || hasSearchQueriesCount

                    // ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šè©³ç´°ã‚’æ–‡å­—åˆ—ã§å‡ºåŠ›
                    console.log(`[vercel.ts] ğŸ” ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºè©³ç´°:`, {
                      hasGroundingMetadata,
                      hasWebSearchQueries,
                      hasSearchQueriesCount,
                      hasSearchGrounding,
                      useSearchGrounding: opts.useSearchGrounding
                    })
                    if (metadata?.google?.groundingMetadata) {
                      console.log(`[vercel.ts] ğŸ“Š groundingMetadataå­˜åœ¨:`, {
                        metadata: JSON.stringify(metadata.google.groundingMetadata).substring(0, 500),
                        webSearchQueries: metadata.google.groundingMetadata.webSearchQueries
                      })
                    }
                    if (usage) {
                      console.log(`[vercel.ts] ğŸ“Š usageå­˜åœ¨:`, {
                        searchQueriesCount: usage.searchQueriesCount,
                        usage: JSON.stringify(usage).substring(0, 300)
                      })
                    }

                    if (hasSearchGrounding) {
                      console.log('[vercel.ts] âœ… ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºæˆåŠŸï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹æ™‚ï¼‰:', {
                        useSearchGrounding: opts.useSearchGrounding,
                        hasSearchGrounding,
                        hasGroundingMetadata: !!metadata?.google?.groundingMetadata,
                        hasWebSearchQueries: !!(metadata?.google?.groundingMetadata?.webSearchQueries && metadata.google.groundingMetadata.webSearchQueries.length > 0),
                        webSearchQueriesCount: metadata?.google?.groundingMetadata?.webSearchQueries?.length || 0,
                        searchQueriesCount: usage?.searchQueriesCount
                      })
                    } else {
                      console.log('[vercel.ts] âŒ ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æœªæ¤œå‡ºï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹æ™‚ï¼‰:', {
                        useSearchGrounding: opts.useSearchGrounding,
                        hasSearchGrounding: false,
                        hasGroundingMetadata: !!metadata?.google?.groundingMetadata,
                        hasWebSearchQueries: !!(metadata?.google?.groundingMetadata?.webSearchQueries && metadata.google.groundingMetadata.webSearchQueries.length > 0),
                        webSearchQueriesCount: metadata?.google?.groundingMetadata?.webSearchQueries?.length || 0,
                        searchQueriesCount: usage?.searchQueriesCount
                      })
                    }
                  } catch (error) {
                    console.error('[vercel.ts] ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ¤œå‡ºã‚¨ãƒ©ãƒ¼ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ é–‹å§‹æ™‚ï¼‰:', error)
                  }
                }

                // ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®æƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦é€ä¿¡
                // ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚‚å«ã‚ã‚‹
                const responseMetadata: any = {
                  messageId: messageId++,
                  hasSearchGrounding: hasSearchGrounding,
                  debug: {
                    useSearchGrounding: opts.useSearchGrounding,
                    optsKeys: Object.keys(opts),
                    optsFull: JSON.stringify(opts).substring(0, 500),
                    hasGroundingMetadata: !!metadata?.google?.groundingMetadata,
                    hasWebSearchQueries: !!(metadata?.google?.groundingMetadata?.webSearchQueries && metadata.google.groundingMetadata.webSearchQueries.length > 0),
                    webSearchQueriesCount: metadata?.google?.groundingMetadata?.webSearchQueries?.length || 0,
                    searchQueriesCount: usage?.searchQueriesCount,
                    hasDynamicRetrievalConfig: !!opts.dynamicRetrievalConfig
                  }
                }
                controller.enqueue(encoder.encode(`f:${JSON.stringify(responseMetadata)}\n`))
                if (hasSearchGrounding) {
                  console.log('[vercel.ts] ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æˆåŠŸã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡')
                } else {
                  console.log('[vercel.ts] ã‚µãƒ¼ãƒã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æœªæ¤œå‡ºã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±å«ã‚€ï¼‰')
                  console.log('[vercel.ts] optsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ:', JSON.stringify(opts, null, 2))
                }

                while (true) {
                  const { done, value } = await reader.read()
                  if (done) {
                    // çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
                    controller.enqueue(encoder.encode(`d:{}\n`))
                    controller.close()
                    break
                  }

                  // ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’Data Streamå½¢å¼ã§é€ä¿¡
                  fullText += value
                  const escapedValue = value.replace(/\\/g, '\\\\').replace(/"/g, '\\"')
                  controller.enqueue(encoder.encode(`0:"${escapedValue}"\n`))
                }
              } catch (error) {
                console.error('[vercel.ts] textStreamå‡¦ç†ã‚¨ãƒ©ãƒ¼:', error)
                // ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
                const errorMessage = error instanceof Error ? error.message : 'An error occurred.'
                controller.enqueue(encoder.encode(`3:"${errorMessage}"\n`))
                controller.close()
              }
            }
          })

          return new Response(dataStream, {
            headers: {
              'Content-Type': 'text/plain; charset=utf-8',
            },
          })
        } catch (streamError: any) {
          console.error('[vercel.ts] streamTextã‚¨ãƒ©ãƒ¼:', streamError)
          console.error('[vercel.ts] ã‚¨ãƒ©ãƒ¼è©³ç´°:', {
            message: streamError?.message,
            stack: streamError?.stack,
            cause: streamError?.cause,
            name: streamError?.name
          })
          // ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ å½¢å¼ã§è¿”ã™
          const errorMessage = streamError?.message || 'An error occurred.'
          return new Response(
            `3:"${errorMessage}"\n`,
            {
              status: 200,
              headers: {
                'Content-Type': 'text/plain; charset=utf-8',
              },
            }
          )
        }
      }

      const result = await generateText({
        model: aiInstance(effectiveModel, opts),
        messages: messages as CoreMessage[],
        temperature,
        maxTokens,
      })

      return new Response(JSON.stringify({ text: result.text }), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      })
    }

    try {
      return await callAI(options)
    } catch (firstError) {
      // Search Grounding ãŒæœ‰åŠ¹ã ã£ãŸå ´åˆã®ã¿ã€ç„¡åŠ¹ã«ã—ã¦å†è©¦è¡Œ
      if (options.useSearchGrounding) {
        console.warn('[vercel.ts] Search Grounding failed, retrying without search...', firstError)
        delete options.useSearchGrounding
        delete options.dynamicRetrievalConfig
        return await callAI(options)
      }
      throw firstError
    }

  } catch (error) {
    console.error('Error in Gemini API call:', error)

    return new Response(
      JSON.stringify({
        error: 'Unexpected Error',
        errorCode: 'AIAPIError',
      }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      }
    )
  }
}

